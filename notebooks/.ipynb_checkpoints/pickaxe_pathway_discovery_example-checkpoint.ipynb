{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fcdde5",
   "metadata": {},
   "source": [
    "#### This notebook is divided into sections. Only section 0 requires inputs from the user, such as the precursor compound to expand, the target metabolite to expand towards, as well as the number of reaction steps to use. The remaining sections don't require external inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46c6f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Key dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02999df7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from minedatabase.pickaxe import Pickaxe\n",
    "from minedatabase.filters import (SimilarityFilter,SimilaritySamplingFilter)\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import utils\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "import rdkit_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c9f6ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show, save\n",
    "from bokeh.models import Range1d, Circle, ColumnDataSource, MultiLine\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.plotting import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fdf57ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read in all compounds present in KEGG, BRENDA, and METACYC\n",
    "# Compounds are in their canonicalized SMILES form and without stereochemistry\n",
    "\n",
    "biological_compounds = set(line.strip() for line in open('../data/all_known_metabolites.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c82331",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Weight function for sampling intermediate metabolites by tanimoto similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516af3b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weight(score):\n",
    "    \"\"\"weight is a function that accepts a similarity score as the sole argument\n",
    "    and returns a scaled value.\n",
    "    \"\"\"\n",
    "    return score ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4201968",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Existing reaction feasibiilty ML model (new one currently in dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac01b2f",
   "metadata": {},
   "source": [
    "This is a reaction prediction model built by Joseph Ni on the XGBoost architecture. The model accepts a substrate's and prodcut's SMILES string as an input, along with the generalized reaction rule that interconverts the two. Subsequently, the model returns a probability score for how likely the monosubstrate enzymatic reaction would be. This works for dimerization reaction too. A new model is currently in development by Yash Chainaini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3e54cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PickXGBClassifier:\n",
    "    \"\"\"XGBoost model to predict feasibility of novel enzymatic reactions enumerated by Pickaxe\"\"\"\n",
    "\n",
    "    def __init__(self, model_path, rules_path):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_path : str\n",
    "            path to pickled enzymatic reaction feasibility classifier\n",
    "        rules_path : str\n",
    "            path to JN1224min ruleset\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = pickle.load(open(model_path, 'rb'))\n",
    "        self.rules_df = pd.read_csv(rules_path, sep='\\t', index_col=0)\n",
    "        self._bondchange_dict = {}\n",
    "        self.bondchange_featurization = lambda s: self._bondchange_lambda(s)\n",
    "        self.compound_featurization = lambda s: self._compound_lambda(s)\n",
    "\n",
    "    def predict_feasibility(self, reactant, product, rule, cutoff=0.5, return_proba=False):\n",
    "        \"\"\"\n",
    "        Return feasibility or feasibility score of novel enzymatic reactions\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reactant : str\n",
    "            reactant SMILES\n",
    "        product : str\n",
    "            product SMILES\n",
    "        rule : str\n",
    "            rule id (for example 'rule0002')\n",
    "        cutoff : float\n",
    "            default 0.5, feasibility score above cutoff will be considered as feasible\n",
    "        return_proba : bool\n",
    "            default False, return feasibility score instead of feasibility if True\n",
    "        \"\"\"\n",
    "\n",
    "        reaction_bits = np.hstack([self.bondchange_featurization(rule), self.compound_featurization(reactant),\n",
    "                                   self.compound_featurization(product)])\n",
    "        feasibility_score = self.model.predict_proba(reaction_bits.reshape(1, 5120))[0][1]\n",
    "\n",
    "        if return_proba is True:\n",
    "            return feasibility_score\n",
    "        else:\n",
    "            if feasibility_score >= cutoff:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def _bondchange_lambda(self, rule):\n",
    "        \"\"\"Featurize bond change patterns\"\"\"\n",
    "\n",
    "        try:\n",
    "            rxn_array = self._bondchange_dict[rule]\n",
    "        except KeyError:\n",
    "\n",
    "            # extract bond change patterns\n",
    "            lhs_smarts = self.rules_df.loc[rule, 'SMARTS'].split('>>')[0]\n",
    "            rhs_smarts = self.rules_df.loc[rule, 'SMARTS'].split('>>')[1]\n",
    "            lhs_any = rdkit_utils.get_smarts(lhs_smarts)[0][\n",
    "                self.rules_df.loc[rule, 'Reactants'].split(';').index('Any')]\n",
    "            rhs_any = rdkit_utils.get_smarts(rhs_smarts)[0][self.rules_df.loc[rule, 'Products'].split(';').index('Any')]\n",
    "\n",
    "            # ECFP4\n",
    "            lhs_ecfp4 = np.array([int(fp) for fp in DataStructs.cDataStructs.BitVectToText(\n",
    "                Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(lhs_any), 4, nBits=256))])\n",
    "            rhs_ecfp4 = np.array([int(fp) for fp in DataStructs.cDataStructs.BitVectToText(\n",
    "                Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(rhs_any), 4, nBits=256))])\n",
    "            lhs_ap = np.zeros(256)\n",
    "            smarts_nonzero_elements = Chem.rdMolDescriptors.GetHashedAtomPairFingerprint(Chem.MolFromSmiles(lhs_any),\n",
    "                                                                                         nBits=256).GetNonzeroElements()\n",
    "\n",
    "            # Atom Pair\n",
    "            for k, v in smarts_nonzero_elements.items():\n",
    "                lhs_ap[k] = v\n",
    "            rhs_ap = np.zeros(256)\n",
    "            smarts_nonzero_elements = Chem.rdMolDescriptors.GetHashedAtomPairFingerprint(Chem.MolFromSmiles(rhs_any),\n",
    "                                                                                         nBits=256).GetNonzeroElements()\n",
    "            for k, v in smarts_nonzero_elements.items():\n",
    "                rhs_ap[k] = v\n",
    "            rxn_array = np.hstack([lhs_ecfp4, rhs_ecfp4, lhs_ap, rhs_ap])\n",
    "\n",
    "            # store bond change fingerprint\n",
    "            self._bondchange_dict[rule] = rxn_array\n",
    "\n",
    "        return rxn_array\n",
    "\n",
    "    def _compound_lambda(self, smiles):\n",
    "        \"\"\"Featurize reactant or product\"\"\"\n",
    "\n",
    "        smiles_fp_array = np.zeros(2 * 1024, dtype=float)\n",
    "\n",
    "        # ECFP4\n",
    "        smiles_fp_array[0:1024] = [int(fp) for fp in DataStructs.cDataStructs.BitVectToText(\n",
    "            Chem.rdMolDescriptors.GetMorganFingerprintAsBitVect(Chem.MolFromSmiles(smiles), 4, nBits=1024))]\n",
    "\n",
    "        # Atom Pair\n",
    "        smiles_nonzero_elements = Chem.rdMolDescriptors.GetHashedAtomPairFingerprint(Chem.MolFromSmiles(smiles),\n",
    "                                                                                     nBits=1024).GetNonzeroElements()\n",
    "        for k, v in smiles_nonzero_elements.items():\n",
    "            smiles_fp_array[k + 1024] = v\n",
    "\n",
    "        return list(smiles_fp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7882f539",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    input_model_path = '../models/PickXGB_model.dat'\n",
    "    input_rules_path = '../data/coreactants_and_rules/JN1224MIN_rules.tsv'\n",
    "    PX = PickXGBClassifier(input_model_path, input_rules_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5593f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 0: Enter expansion parameters (be sure to check everything in these cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81098d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set unique expansion ID and describe this expansion\n",
    "\n",
    "exp_ID = 'pickaxe_example'\n",
    "remarks = 'Demonstrating a pickaxe run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409eea52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enter precursor and target names as well as their SMILES string\n",
    "\n",
    "precursor_name = '2,4,6-trihydroxybenzoic_acid'\n",
    "precursor_smiles = 'O=C(O)c1c(O)cc(O)cc1O'\n",
    "\n",
    "target_name = 'phloroglucinol'\n",
    "target_smiles = 'Oc1cc(O)cc(O)c1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de09f299",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set number of generations and processes (cores) to use for biochemical network expansion\n",
    "\n",
    "num_generations = 2\n",
    "num_processes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac49ea2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set similarity sampling filter and number of compounds to sample per gen\n",
    "# A similarity distribution is first created with a skew towards metabolites most similar to the target\n",
    "# Then the number of compounds in num_samples are sampled to move onto the next generation and the remaining discarded\n",
    "\n",
    "similarity_sample = False\n",
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6330738c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set similarity thresholds and values to cutoff compounds at per gen\n",
    "\n",
    "similarity_filter = False\n",
    "increasing_similarity = False # leave on False, then compounds don't need to strictly increase in similarity\n",
    "\n",
    "similarity_threshold = [0, 0, 0, 0, 0] # default butanetriol pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240f6dc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pick cofactors to expand network with\n",
    "\n",
    "coreactant_list = '../data/coreactants_and_rules/all_cofactors.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a4f356",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Decide if saving results locally or on a remote MongoDB\n",
    "\n",
    "write_mongo = False\n",
    "write_local = True\n",
    "mongo_conn = '' # leave as blank if no mongo cluster\n",
    "local_dir = '../data/pickaxe_runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b284be5",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Decide if you want to visualize the reaction network as an interactive bokeh plot (will take long time if >2 gens)\n",
    "# This interactive plot is created with the bokeh and networkx libraries\n",
    "\n",
    "generate_interactive_plot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce8bce",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 1: Initialize Pickaxe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0f68a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# canonicalize precursor and target SMILES then write to tsv\n",
    "\n",
    "precursor_smiles = utils.canonicalize_smiles(precursor_smiles)\n",
    "target_smiles = utils.canonicalize_smiles(target_smiles)\n",
    "\n",
    "precursor_filepath = utils.write_cpds_to_tsv(precursor_name,precursor_smiles)\n",
    "target_filepath = utils.write_cpds_to_tsv(target_name,target_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fb2309f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pick the relevant rules for expansion\n",
    "# 'generalized' rules focus only on the reaction center undergoing a given transformation (very promiscuous)\n",
    "# 'intermediate' rules focus not only on the reaction center but also on the surround chemical neighborhoods\n",
    "# thus, 'intermediate' rules are more specific\n",
    "\n",
    "rules_type = 'intermediate' # pick either 'generalized' or 'intermediate'\n",
    "\n",
    "rules_range = None # e.g. 100 selects the top 100 rules \n",
    "specific_rule = None # e.g. 'rule0004' or 'rule0004_03' \n",
    "\n",
    "if rules_range:\n",
    "    assert type(rules_range) == int\n",
    "\n",
    "if specific_rule:\n",
    "    assert type(specific_rule) == str\n",
    "\n",
    "rule_list = utils.pick_rules(rules_type='intermediate', rules_range=rules_range, specific_rule=specific_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01eddc44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Intializing pickaxe object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [10:45:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[10:45:37] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [10:45:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[10:45:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done intializing pickaxe object\n",
      "----------------------------------------\n",
      "\n",
      "1 compounds loaded...\n",
      "(1 after removing stereochemistry)\n",
      "1 target compounds loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize pickaxe object\n",
    "\n",
    "pk = Pickaxe(coreactant_list=coreactant_list, rule_list=rule_list)\n",
    "pk.load_compound_set(compound_file=precursor_filepath) # load input compound in Pickaxe\n",
    "pk.load_targets(target_filepath) # load target compound in Pickaxe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af9b442a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## incorporate filters into pickaxe\n",
    "\n",
    "# Similarity filter\n",
    "# Similarity cutoffs are supplied (in section 0) at each generation\n",
    "# Intermediates with Tanimoto similarity less than this cutoff are discarded\n",
    "# Whilte intermediates with Tanimoto similarity more than this cutoff progress onto the next generation\n",
    "\n",
    "sample_fingerprint_method = \"Morgan\"\n",
    "cutoff_fingerprint_method = \"Morgan\"\n",
    "cutoff_fingerprint_args = {\"radius\": 2}\n",
    "cutoff_similarity_method = \"Tanimoto\"\n",
    "\n",
    "crit_similarity = taniFilter = SimilarityFilter(\n",
    "            crit_similarity=similarity_threshold,\n",
    "            increasing_similarity=increasing_similarity,\n",
    "            fingerprint_method=sample_fingerprint_method,\n",
    "            fingerprint_args=cutoff_fingerprint_args,\n",
    "            similarity_method=cutoff_similarity_method)\n",
    "\n",
    "pk.filters.append(crit_similarity)\n",
    "\n",
    "# Similarity sampling filter\n",
    "# This creates a tanimoto similarity distribution at each generation \n",
    "# This distribution compares the similarity of each intermediate to the target metabolite\n",
    "# There is a skew towards intermediates most similar to the target\n",
    "# So that these intermediates are sampled and progressed onto the next generation\n",
    "\n",
    "sample_size = num_samples\n",
    "sample_fingerprint_method = \"Morgan\"\n",
    "sample_fingerprint_args = {\"radius\": 2}\n",
    "sample_similarity_method = \"Tanimoto\"\n",
    "\n",
    "if similarity_sample:\n",
    "    taniSampleFilter = SimilaritySamplingFilter(\n",
    "        sample_size=sample_size,\n",
    "        weight=weight,\n",
    "        fingerprint_method=sample_fingerprint_method,\n",
    "        fingerprint_args=sample_fingerprint_args,\n",
    "        similarity_method=sample_similarity_method)\n",
    "    pk.filters.append(taniSampleFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db75b730",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save all details about this expansion locally or on MongoDB\n",
    "\n",
    "expansion_details = {\"expansion_ID\":exp_ID,\n",
    "                     \"precursor_name\":precursor_name,\n",
    "                     \"precursor_SMILES\":precursor_smiles,\n",
    "                     \"target_name\":target_name,\n",
    "                     \"target_SMILES\":target_smiles,\n",
    "                     \"num_generations\":num_generations,\n",
    "                     \"num_processes\":num_processes,\n",
    "                     \"rules_type\":rules_type,\n",
    "                     \"rules_range\":rules_range,\n",
    "                     \"specific_rule\":specific_rule,\n",
    "                     \"similarity_sample\":similarity_sample,\n",
    "                     \"similarity_sampling_size\":num_samples,\n",
    "                     \"similarity_file\":similarity_filter,\n",
    "                     \"similarity_thresholds\":similarity_threshold,\n",
    "                     \"remarks\":remarks}\n",
    "\n",
    "if write_local:\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(f\"../data/pickaxe_runs/{exp_ID}\")\n",
    "        \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    with open(f\"../data/pickaxe_runs/{exp_ID}/{exp_ID}_expansion_details.json\",\"w\") as outfile:\n",
    "        json.dump(expansion_details,outfile)\n",
    "        \n",
    "if write_mongo:\n",
    "    \n",
    "    # Connect to MongoDB, then create a db and collection\n",
    "    mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "    this_expansion_db = mongo_client[exp_ID]\n",
    "    exp_details_col = this_expansion_db['expansion_details']\n",
    "    docs_alr_present = exp_details_col.find({})\n",
    "    \n",
    "    # Check if there are any documents already present in this collection\n",
    "    i = 0\n",
    "    for doc in docs_alr_present:\n",
    "        i += 1\n",
    "    \n",
    "    if i >= 1:\n",
    "        raise Exception(\"Please define a different expansion ID. This one already exists\")\n",
    "\n",
    "    else:\n",
    "        exp_details_col.insert_one(expansion_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2cb4d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 2: Perform Pickaxe expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "208c3daf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Filtering Generation 0\n",
      "\n",
      "Applying filter: Similarity Cutoff\n",
      "Filtering Generation 0 with similarity > 0.\n",
      "Similarity filter progress: 0 percent complete\n",
      "Similarity filter progress: 100 percent complete\n",
      "1 of 1 compounds selected after Similarity filtering of generation 0 at cutoff of 0. --took 0.03s.\n",
      "\n",
      "Done filtering Generation 0\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Expanding Generation 1\n",
      "\n",
      "Generation 1: 0 percent complete\n",
      "Generation 1 finished in 6.280346870422363 s and contains:\n",
      "\t\t62 new compounds\n",
      "\t\t75 new reactions\n",
      "\n",
      "Done expanding Generation: 1.\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Filtering Generation 1\n",
      "\n",
      "Applying filter: Similarity Cutoff\n",
      "Filtering Generation 1 with similarity > 0.\n",
      "Similarity filter progress: 0 percent complete\n",
      "Similarity filter progress: 10 percent complete\n",
      "Similarity filter progress: 19 percent complete\n",
      "Similarity filter progress: 29 percent complete\n",
      "Similarity filter progress: 39 percent complete\n",
      "Similarity filter progress: 48 percent complete\n",
      "Similarity filter progress: 58 percent complete\n",
      "Similarity filter progress: 68 percent complete\n",
      "Similarity filter progress: 77 percent complete\n",
      "Similarity filter progress: 87 percent complete\n",
      "Similarity filter progress: 97 percent complete\n",
      "Similarity filter progress: 100 percent complete\n",
      "62 of 62 compounds selected after Similarity filtering of generation 1 at cutoff of 0. --took 0.04s.\n",
      "\n",
      "Done filtering Generation 1\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Expanding Generation 2\n",
      "\n",
      "Generation 2: 0 percent complete\n",
      "Generation 2: 10 percent complete\n",
      "Generation 2: 19 percent complete\n",
      "Generation 2: 29 percent complete\n",
      "Generation 2: 39 percent complete\n",
      "Generation 2: 48 percent complete\n",
      "Generation 2: 58 percent complete\n",
      "Generation 2: 68 percent complete\n",
      "Generation 2: 77 percent complete\n",
      "Generation 2: 87 percent complete\n",
      "Generation 2: 97 percent complete\n",
      "Generation 2 finished in 217.94791769981384 s and contains:\n",
      "\t\t6262 new compounds\n",
      "\t\t8680 new reactions\n",
      "\n",
      "Done expanding Generation: 2.\n",
      "----------------------------------------\n",
      "\n",
      "----------------------------------------\n",
      "Filtering Generation 2\n",
      "\n",
      "Applying filter: Similarity Cutoff\n",
      "Filtering Generation 2 with similarity > 0.\n",
      "Similarity filter progress: 0 percent complete\n",
      "Similarity filter progress: 10 percent complete\n",
      "Similarity filter progress: 20 percent complete\n",
      "Similarity filter progress: 30 percent complete\n",
      "Similarity filter progress: 40 percent complete\n",
      "Similarity filter progress: 50 percent complete\n",
      "Similarity filter progress: 60 percent complete\n",
      "Similarity filter progress: 70 percent complete\n",
      "Similarity filter progress: 80 percent complete\n",
      "Similarity filter progress: 90 percent complete\n",
      "Similarity filter progress: 100 percent complete\n",
      "Similarity filter progress: 100 percent complete\n",
      "6262 of 6262 compounds selected after Similarity filtering of generation 2 at cutoff of 0. --took 1.34s.\n",
      "\n",
      "Done filtering Generation 2\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pk.transform_all(generations=num_generations,processes=num_processes)\n",
    "pk.assign_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28dae337",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save Pickaxe compounds and reactions\n",
    "if write_local:\n",
    "    \n",
    "    utils.save_pk_rxns_locally(pk, exp_ID)\n",
    "    utils.save_pk_cpds_locally(pk, exp_ID)\n",
    "\n",
    "if write_mongo:\n",
    "\n",
    "    mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "    this_expansion_db = mongo_client[exp_ID]\n",
    "    compounds_col = this_expansion_db['compounds']\n",
    "    reactions_col = this_expansion_db['reactions']\n",
    "\n",
    "    for compound in pk.compounds:\n",
    "        compounds_col.insert_one(pk.compounds[compound])\n",
    "\n",
    "    for reaction in pk.reactions:\n",
    "        reactions_col.insert_one(pk.reactions[reaction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72890e5c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 3: Extract compounds from Pickaxe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04aeee01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "compounds_df = utils.create_compounds_df(pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62db38a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 4: Extract reactions from Pickaxe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2a669b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pk_rxn_keys = [key for key in pk.reactions.keys()]\n",
    "\n",
    "all_pk_rxn_ids = [pk.reactions[key]['ID'] for key in pk_rxn_keys]\n",
    "all_rxn_strs_in_cpd_ids = [pk.reactions[key]['ID_rxn'] for key in pk_rxn_keys]\n",
    "all_rxn_strs_in_SMILES = [pk.reactions[key]['SMILES_rxn'] for key in pk_rxn_keys]\n",
    "all_rxn_rules = [list(pk.reactions[key]['Operators']) for key in pk_rxn_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86628f84",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 5: Use extracted reactions and pickaxe object to create a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5602679",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "G = utils.create_graph(all_rxn_strs_in_cpd_ids, precursor_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4b202ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this if interested in visualizing network expansion\n",
    "# Will produce an interactive Bokeh plot to visualize nodes and edges\n",
    "# But this takes a lot of time (only really try if <=2 generations)\n",
    "#utils.visualize_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25960ca7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 6: Get sequences from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "445d4f9f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sequences = utils.get_sequences_from_graph(G, compounds_df, precursor_smiles, target_smiles, num_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18bdaf61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_sequences_dict = {}\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    seq_SMILES = [\n",
    "        list(compounds_df[compounds_df[\"ID\"] == id][\"SMILES\"])[0] for id in seq\n",
    "    ]\n",
    "\n",
    "    all_sequences_dict.update(\n",
    "        {f\"seq {i}\": {\"seq_num\": str(i), \"seq (IDs)\": seq, \"seq (SMILES)\": seq_SMILES}}\n",
    "    )\n",
    "\n",
    "if write_local:\n",
    "    with open(f\"../data/pickaxe_runs/{exp_ID}/{exp_ID}_sequences.json\", \"w\") as outfile:\n",
    "        json.dump(all_sequences_dict, outfile)\n",
    "\n",
    "if write_mongo:\n",
    "    mongo_client = pymongo.MongoClient(mongo_conn)\n",
    "    this_expansion_db = mongo_client[exp_ID]\n",
    "    seq_col = this_expansion_db[\"sequences\"]\n",
    "\n",
    "    for key in all_sequences_dict.keys():\n",
    "        seq_col.insert_one(all_sequences_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bfe23da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2,4,6-trihydroxybenzoic_acid', 'pkc0000035', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000038', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000040', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000011', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000044', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000009', 'pkc0000059'],\n",
       " ['2,4,6-trihydroxybenzoic_acid', 'pkc0000058', 'pkc0000059']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d2f28",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Section 7: Enumerate all pathways from sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8532e69",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Section 7: Enumerate and store all pathways from sequences\n",
    "utils.get_pathways_from_graph(write_mongo,\n",
    "                            write_local,\n",
    "                            mongo_conn,\n",
    "                            exp_ID,\n",
    "                            sequences,\n",
    "                            biological_compounds,\n",
    "                            compounds_df,\n",
    "                            pk,\n",
    "                            PX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b33e6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### The following is an example of what the cell above looks like when run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7ff74",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seq_num = 0\n",
    "pathway_num = 0\n",
    "all_pathways = {}\n",
    "\n",
    "### Sequence level\n",
    "for seq in sequences:\n",
    "    seq_num += 1\n",
    "    print('')\n",
    "    print(f'This is sequence number {seq_num}:')\n",
    "    all_rxns_in_this_seq = []\n",
    "    print(seq)\n",
    "    print('')\n",
    "    for i in range(len(seq)-1):\n",
    "\n",
    "        pair = seq[i:i+2]\n",
    "\n",
    "        substrate_ID = pair[0] # eg: pkc0000035\n",
    "        product_ID = pair[1] # eg: pkc0000059\n",
    "\n",
    "        # Get reactions that this substrate participates it (index 0 to get values from series object)\n",
    "        substrate_is_reactant_in = list(compounds_df[compounds_df[\"ID\"]==substrate_ID]['Reactant_in'])[0]\n",
    "\n",
    "        # Get product that this product is formed in (index 0 to get values from series object)\n",
    "        product_is_product_in = list(compounds_df[compounds_df[\"ID\"]==product_ID]['Product_in'])[0]\n",
    "\n",
    "        # Quick test to ensure all reaction hash keys are unique\n",
    "        assert len(substrate_is_reactant_in)==len(set(substrate_is_reactant_in))\n",
    "        assert len(product_is_product_in)==len(set(product_is_product_in))\n",
    "\n",
    "        # Get all reactions between this substrate and product\n",
    "        common_rxns = list(set(substrate_is_reactant_in).intersection(product_is_product_in))\n",
    "        print(f'These are the common reactions between {substrate_ID} and {product_ID}:')\n",
    "        print(common_rxns)\n",
    "        print('')\n",
    "\n",
    "        # store these reactions\n",
    "        all_rxns_in_this_seq.append(common_rxns)\n",
    "\n",
    "    ### Pathway level\n",
    "    # Get all combinations of reactions between these metabolites\n",
    "    all_pathways_in_this_seq = list(itertools.product(*all_rxns_in_this_seq))\n",
    "\n",
    "    print('Combining these common reactions into a list of lists, these are all the reactions in this sequence:')\n",
    "    print(all_rxns_in_this_seq)\n",
    "    print('')\n",
    "    for pathway in all_pathways_in_this_seq:\n",
    "\n",
    "        pathway_num += 1\n",
    "        pathway_rxn_hashes = []\n",
    "        pathway_rxns_in_SMILES = []\n",
    "        pathway_rxns_in_cpd_IDs = []\n",
    "        pathway_rxn_rules = []\n",
    "\n",
    "        print(f'Pathway number {pathway_num} for this sequence is:')\n",
    "\n",
    "\n",
    "        print(pathway)\n",
    "\n",
    "        print('')\n",
    "        print(f'The reactions in pathway number {pathway_num} are:')\n",
    "\n",
    "        for rxn_hash in pathway:\n",
    "\n",
    "            # extract pickaxe reaction dict for this reaction in the pathway\n",
    "            pk_rxn_dict = pk.reactions[rxn_hash]\n",
    "\n",
    "            # extract the reaction rule for this reaction in the pathway\n",
    "            rxn_rules = list(pk_rxn_dict['Operators'])\n",
    "\n",
    "            if len(rxn_rules) == 1:\n",
    "                rxn_rules = rxn_rules[0]\n",
    "\n",
    "            else:\n",
    "                rxn_rules = rxn_rules[0] + ';' + rxn_rules[1]\n",
    "\n",
    "            # extract the reaction string in terms of compound SMILES for this reaction in the pathway\n",
    "            rxn_str_in_SMILES = pk_rxn_dict['SMILES_rxn']\n",
    "\n",
    "            # extract the reaction string in terms of compound IDs for this reaction in the pathway\n",
    "            rxn_str_in_cpd_IDs = pk_rxn_dict['ID_rxn']\n",
    "\n",
    "            # update lists for this pathway\n",
    "            pathway_rxn_hashes.append(rxn_hash)\n",
    "            pathway_rxns_in_SMILES.append(rxn_str_in_SMILES)\n",
    "            pathway_rxns_in_cpd_IDs.append(rxn_str_in_cpd_IDs)\n",
    "            pathway_rxn_rules.append(rxn_rules)\n",
    "\n",
    "            print('')\n",
    "            print(rxn_hash)\n",
    "            print(rxn_str_in_cpd_IDs)\n",
    "            print(rxn_str_in_SMILES)\n",
    "\n",
    "        # Check how many intermediates in sequence were reported in BRENDA, KEGG, or METACYC\n",
    "        num_intermediates_total,\n",
    "        num_known_intermediates,\n",
    "        proportion_known_intermediates,\n",
    "        known_intermediates_or_not = utils.get_seq_info(seq, biological_compounds, compounds_df)\n",
    "\n",
    "        # Extract compounds in sequence\n",
    "        all_seq_cpd_IDs = []\n",
    "        all_seq_cpd_SMILES = []\n",
    "\n",
    "        for cpd_ID in seq:\n",
    "            cpd_SMILES = list(compounds_df[compounds_df[\"ID\"]==cpd_ID][\"SMILES\"])[0]\n",
    "            all_seq_cpd_IDs.append(cpd_ID)\n",
    "            all_seq_cpd_SMILES.append(cpd_SMILES)\n",
    "\n",
    "        # Extract compounds in a pairwise fashion for this sequence again to calculate feasibility scores\n",
    "        for i in range(len(seq)-1):\n",
    "            pair = seq[i:i+2]\n",
    "\n",
    "        pathway_dict = {\"pathway_num\" : pathway_num,\n",
    "                    \"sequence_num\" : seq_num,\n",
    "                    \"sequence (compound IDs)\" : all_seq_cpd_IDs,\n",
    "                    \"sequence (SMILES)\" : all_seq_cpd_SMILES,\n",
    "                    \"reactions (SMILES)\" : pathway_rxns_in_SMILES,\n",
    "                    \"reactions (compound IDs)\" : pathway_rxns_in_cpd_IDs,\n",
    "                    \"reaction rules\" : pathway_rxn_rules,\n",
    "                    \"num_intermediates\" : num_intermediates_total,\n",
    "                    \"num_known_intermediates\" : num_known_intermediates,\n",
    "                    \"proportion_known_intermediates\" : proportion_known_intermediates,\n",
    "                    \"type_known_intermediates\" : known_intermediates_or_not}\n",
    "\n",
    "        print('')\n",
    "        print(pathway_dict)\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "    print('----------------------------------------')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-my-rdkit-env]",
   "language": "python",
   "name": "conda-env-anaconda3-my-rdkit-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
